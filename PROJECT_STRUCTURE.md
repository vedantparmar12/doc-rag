# MCP Docs RAG - Project Structure

## Complete File Layout

```
mcp-docs-rag/
â”‚
â”œâ”€â”€ ğŸ“„ README.md                    # Complete documentation
â”œâ”€â”€ ğŸ“„ QUICKSTART.md                # 5-minute setup guide
â”œâ”€â”€ ğŸ“„ USAGE.md                     # Usage examples and best practices
â”œâ”€â”€ ğŸ“„ SETUP_FOR_COPILOT.md         # GitHub Copilot setup guide
â”œâ”€â”€ ğŸ“„ SUMMARY.md                   # Quick overview
â”œâ”€â”€ ğŸ“„ PROJECT_STRUCTURE.md         # This file
â”‚
â”œâ”€â”€ ğŸ“„ pyproject.toml               # Dependencies
â”œâ”€â”€ ğŸ“„ .env.example                 # Environment template
â”œâ”€â”€ ğŸ“„ server.py                    # MCP server entry point
â”‚
â”œâ”€â”€ search/                         # Search engine
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hybrid_search.py            # Multi-strategy search engine
â”‚   â””â”€â”€ embedders.py                # Embedding providers (HF, OpenAI)
â”‚
â”œâ”€â”€ tools/                          # MCP tool implementations
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ search_tools.py             # search_docs, semantic_search, find_configs
â”‚   â”œâ”€â”€ file_tools.py               # get_file, list_structure
â”‚   â””â”€â”€ summarize_tools.py          # summarize_topic
â”‚
â”œâ”€â”€ indexer/                        # Document indexing
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ doc_indexer.py              # Index builder with Docling
â”‚   â””â”€â”€ build_index.py              # CLI for indexing
â”‚
â””â”€â”€ config/                         # Configuration examples
    â”œâ”€â”€ vscode-settings.json        # VS Code MCP config
    â””â”€â”€ claude-desktop-config.json  # Claude Desktop config
```

## Key Components

### 1. **server.py** - MCP Server
```python
# Entry point for MCP server
# Registers 6 tools:
#   - search_docs
#   - semantic_search
#   - get_file
#   - list_structure
#   - summarize_topic
#   - find_configs
```

### 2. **search/embedders.py** - Embedding Engine
```python
# Supports 3 providers:
#   - local: HuggingFace (FREE!)
#   - huggingface: HF API
#   - openai: OpenAI API
```

### 3. **search/hybrid_search.py** - Search Engine
```python
# Multi-strategy search:
#   1. Exact match (instant)
#   2. Ripgrep search (10-50ms)
#   3. Fuzzy match (50ms)
#   4. Semantic search (200ms)
```

### 4. **indexer/doc_indexer.py** - Indexer
```python
# Builds searchable index:
#   - Scans all .md files
#   - Extracts metadata
#   - Generates embeddings
#   - Saves to .index/
```

### 5. **tools/** - MCP Tools
```python
# 6 tools for documentation search:
#   search_tools.py    - Fast & semantic search
#   file_tools.py      - File operations
#   summarize_tools.py - AI summarization
```

## Data Flow

```
1. BUILD INDEX (one-time)

   .md files â†’ doc_indexer.py â†’ .index/
                    â†“
            embedders.py (HF local)
                    â†“
            index.json + embeddings.npy


2. RUNTIME SEARCH

   User query â†’ server.py â†’ hybrid_search.py
                                   â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â–¼                             â–¼
              Fast Search                  Semantic Search
              (ripgrep)                    (embeddings)
                    â†“                             â†“
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                   â–¼
                            Top 5-10 results
                                   â†“
                            GitHub Copilot
```

## Configuration Files

### .env (Your Settings)
```bash
DOCS_FOLDER=/path/to/docs
EMBEDDING_PROVIDER=local
EMBEDDING_MODEL=all-MiniLM-L6-v2
ENABLE_SEMANTIC_SEARCH=true
```

### VS Code settings.json
```json
{
  "mcp.servers": {
    "docs-rag": {
      "command": "uv",
      "args": ["run", "python", "/path/to/server.py"],
      "env": {
        "DOCS_FOLDER": "/path/to/docs"
      }
    }
  }
}
```

## Generated Files

```
mcp-docs-rag/
â”œâ”€â”€ .index/                  # Generated by indexer
â”‚   â”œâ”€â”€ index.json           # File metadata (titles, headings, etc.)
â”‚   â””â”€â”€ embeddings.npy       # Vector embeddings (384 or 768 dim)
â”‚
â””â”€â”€ .env                     # Your configuration (not in git)
```

## Dependencies

### Core (Required)
- `mcp` - Model Context Protocol
- `sentence-transformers` - Local HF embeddings
- `docling` - Document processing
- `numpy` - Vector operations
- `rapidfuzz` - Fuzzy matching
- `aiohttp` - Async HTTP

### Optional
- `openai` - If using OpenAI embeddings
- `torch` - GPU acceleration for embeddings

## File Sizes (Approximate)

| File | Lines | Purpose |
|------|-------|---------|
| server.py | 200 | MCP server setup |
| hybrid_search.py | 500 | Search engine logic |
| embedders.py | 300 | Embedding providers |
| doc_indexer.py | 300 | Index builder |
| search_tools.py | 200 | Search MCP tools |
| file_tools.py | 150 | File MCP tools |
| summarize_tools.py | 100 | Summarization tool |

**Total:** ~1800 lines of Python

## Memory Usage

| Component | Memory | Notes |
|-----------|--------|-------|
| Index (3000 files) | ~50-100 MB | Stored on disk |
| Embeddings (3000 files) | ~50-100 MB | Stored on disk |
| Runtime | ~500 MB | Embedding model in RAM |
| Per search | ~10 MB | Temporary |

## Performance

| Operation | Time | Details |
|-----------|------|---------|
| Server startup | 2-3 sec | Load embedding model |
| Index load | 1-2 sec | Load from disk |
| Keyword search | 10-50 ms | Ripgrep |
| Semantic search | 200-500 ms | Local embeddings |
| Summarization | 2-5 sec | Uses LLM (Copilot) |

## Error Handling

Each module includes:
- âœ… Graceful fallbacks (semantic â†’ keyword)
- âœ… Error logging
- âœ… User-friendly error messages
- âœ… Timeout handling
- âœ… File not found handling

## Extensibility

Easy to add:
1. **New search strategy** â†’ Add to `hybrid_search.py`
2. **New embedding provider** â†’ Add to `embedders.py`
3. **New MCP tool** â†’ Create in `tools/` and register in `server.py`
4. **New doc format** â†’ Use Docling's converter in `doc_indexer.py`

## Testing

```bash
# Test embedding options
uv run python search/embedders.py

# Test search (after building index)
uv run python -c "
import asyncio
from search.hybrid_search import HybridSearchEngine
from pathlib import Path

async def test():
    engine = HybridSearchEngine(
        docs_folder=Path('your-docs'),
        index_path=Path('.index'),
        enable_semantic=True
    )
    await engine.initialize()
    results = await engine.fast_search('test query')
    print(results)

asyncio.run(test())
"
```

## Development Workflow

```bash
# 1. Edit code
vim search/hybrid_search.py

# 2. Rebuild index if needed
uv run python -m indexer.build_index

# 3. Test server
uv run python server.py

# 4. Test in VS Code
@docs-rag search for "test"
```

## Git Ignore

```gitignore
.env
.index/
__pycache__/
*.pyc
.pytest_cache/
*.log
```

## Next Steps

1. **Setup:** Follow [SETUP_FOR_COPILOT.md](SETUP_FOR_COPILOT.md)
2. **Use:** Read [USAGE.md](USAGE.md)
3. **Customize:** Modify `search/` or `tools/` as needed
4. **Deploy:** Use `server.py` with your docs folder

**Questions?** See [README.md](README.md) or open an issue!
